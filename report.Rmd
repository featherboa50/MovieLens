---
title: "Movielens Report"
author: "Ariel Cooper"
date: "`r Sys.Date()`"
output:
  pdf_document: default
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# MovieLens Capstone

```{r packages, echo = FALSE, message=FALSE, warning=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "https://cran.rstudio.com/")
if(!require(caret)) install.packages("caret", repos = "https://cran.rstudio.com/")
if(!require(kamila)) install.packages("kamila", repos = "https://cran.rstudio.com/")
if(!require(recipes)) install.packages("recipes", repos = "https://cran.rstudio.com/")
if(!require(kableExtra)) install.packages("kableExtra", repos = "https://cran.rstudio.com/")

library(splitstackshape)
library(tidyverse)
library(caret)
library(ggplot2)
library(kamila)
library(readr)
library(recipes)
library(lubridate)
library(kableExtra)
```

```{r setup, echo = FALSE, message=FALSE, warning=FALSE}

options(timeout = 120)

dl <- "ml-10M100K.zip"
if(!file.exists(dl))
  download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings_file <- "ml-10M100K/ratings.dat"
if(!file.exists(ratings_file))
  unzip(dl, ratings_file)

movies_file <- "ml-10M100K/movies.dat"
if(!file.exists(movies_file))
  unzip(dl, movies_file)

# had to adjust due to fixed() erroring out
ratings <- as.data.frame(str_split(read_lines(ratings_file), coll("::"), simplify = TRUE),
                         stringsAsFactors = FALSE)
colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")
ratings <- ratings %>%
  mutate(userId = as.integer(userId),
         movieId = as.integer(movieId),
         rating = as.numeric(rating),
         timestamp = as.integer(timestamp))


movies <- as.data.frame(str_split(read_lines(movies_file), coll("::"), simplify = TRUE),
                        stringsAsFactors = FALSE)
colnames(movies) <- c("movieId", "title", "genres")
movies <- movies %>%
  mutate(movieId = as.integer(movieId))

#Add RELEASE YEAR and time since release to time of rating to both sets
movielens <- left_join(ratings, movies, by = "movieId") %>%
  mutate(release_year = as.numeric(str_sub(title,-5,-2)),
         years_since = ifelse(year(timestamp) - release_year <= 0, 0, year(timestamp) - release_year))

# Final hold-out test set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
# set.seed(1) # if using R 3.5 or earlier
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in final hold-out test set are also in edx set
final_holdout_test <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from final hold-out test set back into edx set
removed <- anti_join(temp, final_holdout_test)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed, movies_file, ratings_file)


```

## Background

When streaming shows online, there will be the ubiquitous recommendations for what to watch next. What you are show is customized to you, enticing you to watch more content tailored to your preferences and past behavior. Along with this the streaming service benefits by getting you to watch more of their content. In 2006, Netflix released a challenge to beat their current algorithms by at least 10%. To the winner would go a prize of \$1,000,000.

To create the recommendations, challengers were given a training set of data of over 100,000,000 rows. Each row included information such as, but not limited to, a user ID, movie title, and a rating from 1-5. Netflix would have a second set of samples of around 2.8 million, which the contestants would need to predict the rating using the training set they were provided. The measure of success would be to get the lowest root mean squared error(RMSE), which will be explained further in the next section. To beat Netflix's algorithm, a RMSE of 0.8563 or lower would need to be achieved. (TÂ¨Oscher, A., & Jahrer, M., 2009)

##### RMSE what is it and how it is calculated?

When trying to calculate how well an algorithm is performing we need some kind of metric to base it off of. Is just guessing randomly better than your carefully crafted code? There are many ways to create this calculation and it all depends on what you are trying to measure and the data it comes from. For this project we will be using RMSE.

This formula is calculating the average distance from predicted values to the actual value. For the most accurate prediction, we want the RMSE to be as low as possible since it is indicating our predicted values are not far from the actual. (Bobbitt, 2021)

$$RMSE = \sqrt{\frac{1}{N}\sum_{u,i}(\hat{y}_{u,i}-y_{u,i})^2}$$

$N$ is the sample size

$u,i$ refers to the index, so in this case it the review for user $u$ and movie $i$

$\hat{y}_{u,i}$ is the predicted rating for user $u$ and movie $i$

${y}_{u,i}$ is the true rating for user $u$ and movie $i$

$\sum_{u,i}$ is the sum of all results over all users and movies

## Exploring the Data

We will be using a subset of the MovieLens data with 1M entries due to time and hardware constraints. 10% of the data will be separated out as our test for accuracy. We cannot use this set of data during creation of our algorithm, similar how we would not know exactly how someone would rate a movie in real life. But we could try to figure it out based on past behavior of the person and others. This mystery subset will only be used to check our results at the end of the testing.

Overall the downloaded movielens data has already been cleaned from any entries contain NULL values. It contains several columns (Table 1).

*Table 1: Original columns provided*

| Column    | Data Type  | Description                                                                                      |
|-------------------|-------------------|----------------------------------|
| userId    | integer    | unique identifier for a person providing ratings                                                 |
| movieId   | integer    | unique identifier for each movie                                                                 |
| title     | chr string | name of the movie                                                                                |
| rating    | number     | between 0-5 in 0.5 increments representing the rating the user has provided. 5 being the highest |
| genres    | chr string | genres of the movie, separated by a '\|' character                                               |
| timestamp | integer    | UNIX timestamp of when the rating was created                                                    |

Two additional columns were created from the existing data to be used later (Table 2).

*Table 2: Created columns*

| Column       | Data Type | Description                                                         |
|-------------------|-------------------|----------------------------------|
| release_year | integer   | year movie was released, extracted from title string                |
| years_since  | integer   | year the rating was created extracted from timestamp - release year |

In the edx table, there are a total of 9,000,055 ratings, from 69,878 users for 10,677 movies. Based on those numbers, not every user is rating every movie. It is only about 1.2% of the total possible ratings, but we would not expect every person to have watched and rated every movie.

```{r explore, echo = TRUE, results='hide'}
# Number of movies
n_distinct(edx$movieId)
# Number of users
n_distinct(edx$userId)
# Total ratings
nrow(edx)
```

Given that not every user is rating everything, the distribution of how frequently a user rates a movie varies highly (Figure 1). Only a small portion of users have more than 100 ratings recorded. Then looking at the spread of the ratings based on the user (Figure 2), it shows that most users will rate a movie between 3-4 stars. This shows a bias that an individual creates and should be accounted for in the algorithm.

```{r, echo=FALSE, warning=FALSE}
edx %>% count(userId) %>% ggplot(aes(n)) + geom_histogram(bins=30, fill="#ffa600", colour="#003f5c", size = .8) + labs(x = "Frequency of Users", y = "Count of Ratings", title = "Figure 1: Frequency of users with a given number of ratings") + xlim(0,3000) + scale_y_log10() + theme_light()
```

```{r, echo=FALSE, warning=FALSE}
 # Code that generates warning messages 
edx %>% group_by(userId) %>% summarize(avg = mean(rating)) %>% ggplot(aes(avg)) + geom_histogram(bins = 10, fill="#ffa600", colour="#003f5c", size = .8) + labs(x = "Average Rating", y = "Count of Users", title = "Figure 2: Frequency of the average rating based on users") + theme_light()
```

Similar to the bias created by a particular user, we can see that a particular movie will have its own trend of most ratings being between 2.5-4 stars(Figure 3). Also we can see that more popular movies(i.e. more ratings) tend to have a higher ratings(Figure 4). This effect should be accounted for when predicting the ratings.

```{r, echo=FALSE, warning=FALSE}
 # Code that generates warning messages 
edx %>% group_by(movieId) %>% summarize(avg = mean(rating)) %>% ggplot(aes(avg)) + geom_histogram(bins = 10, fill="#ffa600", colour="#003f5c", size = .8) + labs(x = "Average Rating", y = "Count of Movies", title = "Figure 3: Frequency of the average rating of a movie") + theme_light()
```

```{r, echo=FALSE, warning=FALSE}
 # Code that generates warning messages 
edx %>% group_by(movieId) %>% summarize(n = n(), avg = mean(rating)) %>% group_by(n) %>% summarize(avg= mean(avg)) %>% ggplot(aes(n,avg)) + geom_point(colour="#003f5c", alpha = 0.1) + geom_smooth(method = lm, formula = y ~ splines::bs(x, 5), se = FALSE, color = "#bc5090", size = 2) + labs(x = "Number of Ratings per Movie", y = "Average Rating", title = "Figure 4: Average rating based on the frequency a movie is rated") + theme_light() + scale_x_log10() 
```

After calculating how long ago the rating is from the release year, a trend is revealed that there is higher ratings as time goes on (Figure 5). Time ranges with less than 1000 ratings were removed due to creating outliers.This trend may be due to if a movie is still getting frequent ratings after the first 5 years, then it could be called a classic and is more highly rated.

```{r years_chart, echo=FALSE, message= FALSE}
#Years since release date for years with over 1000 ratings
edx %>% group_by(years_since) %>% 
  filter(n() > 1000)%>%
  summarise(mean = mean(rating)) %>%
  ggplot(aes(years_since,mean)) +geom_point(colour = ("#003f5c")) +geom_smooth(method = "loess", span = 0.65, se = FALSE, color = "#bc5090", size = 1.5) + theme_light() + labs(x = "Years since Release Date", y = "Average Rating", title = "Figure 5: Comparison of time since release vs the average rating")
```

## Pre-processing

Before splitting up the data between the final holdout test set, two columns were created (Figure 2). The release year and the number of years between the release and the rating.

With the available data it is split up into two again. A training set to give the algorithms something to work off of and a testing set to test RMSE against before applying to our original mystery set.

A list of indices was created for each userId that contained 90% of their ratings in one group and 10% in another. The 10% was used for the testing set, 90% for training. They were then compared to make sure both sets contained the same userId's and movieId's, as if any were missing from the training set it would create issues later on.

```{r test-sets, message= FALSE}
#creates sets of each user and then takes 20% of each user for testing
set.seed(2006)
indexes <- split(1:nrow(edx), edx$userId)
test_ind <- sapply(indexes, function(ind) sample(ind, ceiling(length(ind)*.2))) %>%
  unlist(use.names = TRUE) %>% sort()

training <- edx[-test_ind,]
temp <- edx[test_ind,]

# Make sure same movieId in testing and training
testing <- temp %>% 
  semi_join(training, by = "movieId")

# Add rows removed from final hold-out test set back into edx set
removed <- anti_join(temp, testing)
training <- rbind(training, removed)
```

## Methods tried

#### Guessing

The most basic method used is to guess the same rating for all movies, regardless of any other factors. The best fit for this is to take an average of all ratings in the data. We'll call this variable $\mu$. Putting this in a general formula, it comes out to our prediction $\hat{y}$ for any given rating equaling $\mu$.

$\mu = 3.511909$

The formula being:

$\hat{y} = \mu$

```{r, echo = FALSE, message = FALSE}
#root mean squared error
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}

#base model using the avg for all ratings
mu <- training %>% summarize(m = mean(rating)) %>% pull(m)
rmse_results <- data.frame(Model = "Average Rating", rmse = RMSE(testing$rating, mu))
```

```{r, echo = FALSE}
rmse_results %>% kbl() %>%
  kable_styling()
```

### Movie Bias

Now we know intuitively and from exploring the data before that not every movie will have the same rating. Some have better ratings on average than others and others less than the average. To account for this we will add a new variable $b_i$. This is the bias created by a particular movie $i$. To calculate this we'll see how far from $\mu$ a paritcular movie is on average, and then use that number when making a prediction for that movie. (Irizarry, n.d.)

$Y_i = \mu + b_i$

```{r, echo = FALSE, warning=FALSE}
#basic model using avg rating for the movie
avg_ratings_by_movie <- training %>% group_by(movieId) %>% 
  summarize(b_i = mean(rating) - mu,
            n = n(),
            sums = sum(rating - mu))
temp <- inner_join(testing,avg_ratings_by_movie, by = 'movieId') %>% 
  summarize(RMSE = RMSE(rating, b_i + mu)) %>% pull(RMSE)
rmse_results <- bind_rows(rmse_results,
          data_frame(Model="Movie effect model", rmse = temp ))
rmse_results %>% filter(Model == "Movie effect model") %>% kbl() %>%
  kable_styling()

```

### User Bias

Just as each movie will have its own effect on the rating, each user will have their own effect indicated by $b_u$. For simplicity we just add the two together in the current formula, but later on we will adjust for $b_i$ when calculating $b_u$. By combining the two effects we significant drop in the RMSE, so both the movie and the user are confirmed to be able to be used as predictors.

$\hat{y}_{i,u} = \mu + b_i + b_u$

```{r, echo = FALSE}

#basic model using avg rating by user
#RMSE 0.98
avg_ratings_by_user <- training %>% group_by(userId) %>% 
  summarize(b_u = mean(rating) - mu,
            n = n(),
            sums = sum(rating - mu))


#basic model using avg rating by genre groups
avg_ratings_by_genre <- training %>% group_by(genres) %>% 
  summarize(b_g = mean(rating) - mu,
            n = n(),
            sums = sum(rating - mu))



#basic model using avg rating by release year
avg_ratings_by_release_year <- training %>%
  group_by(release_year) %>% 
  summarize(n = n(),
            sums = sum(rating - mu))


#movie + user bias


temp <- left_join(testing, avg_ratings_by_movie, by = "movieId") %>%
  left_join(avg_ratings_by_user, by = "userId") %>%
  mutate(pred = mu + b_i + b_u) %>% 
  summarize(rmse = RMSE(rating, pred)) %>% pull(rmse)
rmse_results <- bind_rows(rmse_results,
          data_frame(Model="User effect model", rmse = temp ))
rmse_results %>% filter(Model == "User effect model") %>% kbl() %>%
  kable_styling()
```

#### Reg Movie + User

When looking at the top rated movies, the results seem odd, they are not well known titles(Table 3). Looking closer, they only have a single rating. Intuitively we would expect a truely popular movie to have more ratings, so to account for this we are going to adjust $b_i$ and weight it based on the number of reviews $n_i$. Additionally, adjusting with a chosen value $\lambda$, the RMSE can be reduced. To find the ideal $\lambda$, a varitity of values will be tested to find the one that reduces the RMSE the most (Figure 6)

$\hat{b}_i(\lambda) = \frac{1}{n_i + \lambda}\sum(Y_i -\mu)$

```{r lambdas}

#finding the best lambda for smoothing movie effect
lambdas <- seq(0, 3, 0.05)
rmses <- sapply(lambdas, function(lambda){
  avg_ratings_by_movie$b_i <- avg_ratings_by_movie$sums / (avg_ratings_by_movie$n + lambda)
  left_join(testing, avg_ratings_by_movie, by = "movieId") %>% mutate(pred = mu + b_i) %>% 
    summarize(rmse = RMSE(rating, pred)) %>%
    pull(rmse)
})
```

```{r, echo = FALSE}
#plot regularized movie effect
data.frame(lambdas, rmses) %>% ggplot(aes(lambdas, rmses)) + geom_line(color = "#bc5090")+ labs(title = "Figure 6: The RMSEs from the lambdas tested")
```

After finding the ideal $\lambda$ and regularizing $b_i$ we get a new list of the top rated movies. This is more in line with what you would expected (Table 4).

*Table 3: Top rated movies*

```{r echo = FALSE}
training %>% group_by(title) %>% summarise(average_rating = mean(rating), number_of_ratings = n()) %>% arrange(desc(average_rating)) %>% slice(1:4) %>% kbl() %>% kable_styling()
```

Table 4: Top rated movies after regularization

```{r, echo = FALSE}
lambda_i <- lambdas[which.min(rmses)]
avg_ratings_by_movie$b_i <- avg_ratings_by_movie$sums / (avg_ratings_by_movie$n + lambda_i)
#training_reg used to keep track of the b_[] for future regulation formulas
training_reg <- left_join(training,avg_ratings_by_movie, by = "movieId") %>%
  select(-n,-sums)
#movieId and title are interchangable
training_reg %>% group_by(title) %>% summarise(average_rating = mean(rating), number_of_ratings = n(), b_i = first(b_i)) %>% arrange(desc(b_i)) %>% select(title, average_rating, number_of_ratings) %>% slice(1:4)%>% kbl() %>% kable_styling()
```

Now with our new;y regularized $b_i$ we can recalculate $b_u$ with $b_i$ factored in.

```{r}
#adjust b_u to reg. b_i
avg_ratings_by_user <- training_reg %>%
  mutate(x = rating - mu - b_i) %>%
  group_by(userId) %>% 
  summarize(b_u = mean(x),
            n = n(),
            sums = sum(x))

```

```{r reg_movie_rmse, echo = FALSE}

temp <- left_join(testing, avg_ratings_by_movie, by = "movieId") %>%
  left_join(avg_ratings_by_user, by = "userId") %>%
  mutate(pred = mu + b_i + b_u) %>% 
  summarize(rmse = RMSE(rating, pred)) %>% pull(rmse)
rmse_results <- bind_rows(rmse_results,
          data_frame(Model="Reg. Movie + User effect model", rmse = temp ))
rmse_results %>% filter(Model == "Reg. Movie + User effect model") %>% kbl() %>%
  kable_styling()
```

### Reg ALL

```{r reg-setup, echo = FALSE}


#basic model using avg rating by genre groups
avg_ratings_by_genre <- training %>% group_by(genres) %>% 
  summarize(b_g = mean(rating) - mu,
            n = n(),
            sums = sum(rating - mu))


#basic model using avg rating by release year
avg_ratings_by_release_year <- training %>%
  group_by(release_year) %>% 
  summarize(n = n(),
            sums = sum(rating - mu))


#regularize for users
lambdas <- seq(3, 7, 0.25)
rmses <- sapply(lambdas, function(lambda){
  avg_ratings_by_user$b_u <- avg_ratings_by_user$sums / (avg_ratings_by_user$n + lambda)
  left_join(testing, avg_ratings_by_user, by = "userId") %>%
    left_join(avg_ratings_by_movie, by = "movieId") %>%
    mutate(pred = mu + b_u + b_i) %>% 
    summarize(rmse = RMSE(rating, pred)) %>%
    pull(rmse)
})
#qplot(lambdas, rmses, geom = "line")
lambda_u <- lambdas[which.min(rmses)]
avg_ratings_by_user$b_u <- avg_ratings_by_user$sums / (avg_ratings_by_user$n + lambda_u)
training_reg <- left_join(training_reg, avg_ratings_by_user, by = "userId") %>%
  select(-n,-sums)
```

```{r reg-compute, echo = FALSE}



#adj genres for bi&bu before regularization
avg_ratings_by_genre <- training_reg %>%
  mutate(x = rating - mu - b_i - b_u) %>%
  group_by(genres) %>% 
  summarize(n = n(),
            sums = sum(x))
#regularize for genres
lambdas <- seq(24, 28, 0.25)
rmses <- sapply(lambdas, function(lambda){
  avg_ratings_by_genre$b_g <- avg_ratings_by_genre$sums / (avg_ratings_by_genre$n + lambda)
  left_join(testing, avg_ratings_by_genre, by = "genres") %>%
    left_join(avg_ratings_by_movie, by = "movieId") %>% 
    left_join(avg_ratings_by_user, by = "userId") %>%
    mutate(pred = mu + b_u + b_i + b_g) %>% 
    summarize(rmse = RMSE(rating, pred)) %>%
    pull(rmse)
})
#regularized genre groups effect
#qplot(lambdas, rmses, geom = "line")
lambda_g <- lambdas[which.min(rmses)]
#print(lambda_g)
avg_ratings_by_genre$b_g <- avg_ratings_by_genre$sums / (avg_ratings_by_genre$n + lambda_g)
training_reg <- left_join(training_reg, avg_ratings_by_genre, by = "genres") %>%
  select(-n,-sums)



#adj genres for all previous b_[] before regularization
avg_ratings_by_release_year <- training_reg %>%
  mutate(x = rating - mu - b_i - b_u - b_g) %>%
  group_by(release_year) %>% 
  summarize(n = n(),
            sums = sum(x))
#regularize for release year
lambdas <- seq(-18, -14, .25)
rmses <- sapply(lambdas, function(lambda){
  avg_ratings_by_release_year$b_r <- avg_ratings_by_release_year$sums / (avg_ratings_by_release_year$n + lambda)
  left_join(testing, avg_ratings_by_release_year, by = "release_year") %>%
    left_join(avg_ratings_by_movie, by = "movieId") %>% 
    left_join(avg_ratings_by_user, by = "userId") %>% 
    left_join(avg_ratings_by_genre, by = "genres") %>%
    mutate(pred = mu + b_u + b_i + b_g + b_r) %>% 
    summarize(rmse = RMSE(rating, pred)) %>%
    pull(rmse)
})
#regularized release year effect
#qplot(lambdas, rmses, geom = "line")
lambda_r <- lambdas[which.min(rmses)]
#print(lambda_r)
avg_ratings_by_release_year$b_r <- avg_ratings_by_release_year$sums / (avg_ratings_by_release_year$n + lambda_r)
training_reg <- left_join(training_reg, avg_ratings_by_release_year, by = "release_year") %>%
  select(-n,-sums)



#adj years between rating and release 
avg_ratings_by_rating_time <- training_reg %>%
  mutate(x = rating - mu - b_i - b_u - b_g - b_r) %>%
  group_by(years_since) %>% 
  summarize(n = n(),
            sums = sum(x))

lambdas <- seq(99, 102, 0.25)
rmses <- sapply(lambdas, function(lambda){
  avg_ratings_by_rating_time$b_y <- avg_ratings_by_rating_time$sums / (avg_ratings_by_rating_time$n + lambda)
  left_join(testing, avg_ratings_by_rating_time, by = "years_since") %>%
    left_join(avg_ratings_by_movie, by = "movieId") %>% 
    left_join(avg_ratings_by_user, by = "userId") %>% 
    left_join(avg_ratings_by_genre, by = "genres") %>%
    left_join(avg_ratings_by_release_year, by = "release_year") %>%
    mutate(pred = mu + b_u + b_i + b_g + b_r + b_y) %>% 
    summarize(rmse = RMSE(rating, pred)) %>%
    pull(rmse)
})
#regularized years since release effect
#qplot(lambdas, rmses, geom = "line")
lambda_y <- lambdas[which.min(rmses)]
#print(lambda_y)
avg_ratings_by_rating_time$b_y <- avg_ratings_by_rating_time$sums / (avg_ratings_by_rating_time$n + lambda_y)

```

If regularizing for $b_i$ improves the RMSE, it could stand to reason that regularizing all the columns being used would improve the predictions. After exploring the data, the factors that will be used are movie_Id, userId, years_since($b_y$), release_year($b_r$), and genres($b_g$). When regularizing all the factors, it will need to be done one-by-one to account for the columns already regularized so that they work together to build an accurate prediction.

$Y_i = \mu + \hat{b_i} + \hat{b_u} +\hat{b_y} +\hat{b_r} + \hat{b_g}$

```{r rmse_results, echo=FALSE}

rmse_results <- bind_rows(rmse_results,
          data_frame(Model="All Regularized model", rmse = min(rmses) ))
rmse_results %>% filter(Model == "All Regularized model") %>% kbl() %>%
  kable_styling()
```

## Results

After getting the model down to an RMSE of 0.865, it is ready to apply to the final holdout test data. Resulting in the final RMSE of **0.8647925**. A significant improvement over the original 1.06

```{r final}
Final <- left_join(final_holdout_test, avg_ratings_by_movie, by = "movieId") %>%
  select(-n,-sums) %>%
  left_join(avg_ratings_by_user, by = "userId") %>%
  select(-n,-sums) %>%
  left_join(avg_ratings_by_genre, by = "genres") %>%
  select(-n,-sums) %>%
  left_join(avg_ratings_by_release_year, by  = "release_year") %>%
  select(-n,-sums) %>%
  left_join(avg_ratings_by_rating_time, by  = "years_since") %>%
  select(-n,-sums) %>%
  mutate(pred = mu + b_i + b_u + b_g + b_r + b_y) %>%
  summarize(x = RMSE(rating, pred))

```

```{r final_rmse, echo = FALSE}
rmse_results <- bind_rows(rmse_results,
          data_frame(Model="Final Results", rmse = Final$x ))
rmse_results  %>% kbl() %>%
  kable_styling()
```

## Conclusion

The objective of this project was to create a model that would get an RMSE of 0.86490 or less. This one has exceeded that with 0.8647925. Overall the regularization model had a 22% improvement over the generic average rating method.

It was limited however due to restrictions of time and hardware resources available on a personal computer. More advance algorithms would require more resources that could help account for a variety of factors. Improvements could be made by taking into account similar user's ratings or similar movies(such as a sequel to a previously highly rated movie).

## References

TÂ¨Oscher, A., & Jahrer, M. (2009). The BigChaos Solution to the Netflix Grand Prize. InÂ *https://www.asc.ohio-state.edu/statistics/statgen/joul_aut2009/BigChaos.pdf*. AT&T Labs - Research.Â https://www.asc.ohio-state.edu/statistics/statgen/joul_aut2009/BigChaos.pdf

Bobbitt, Z. (2021, May 10).Â *How to interpret Root Mean Square Error (RMSE)*. Statology.Â https://www.statology.org/how-to-interpret-rmse/

Irizarry, R. A. (n.d.).Â *Introduction to data Science*.Â https://rafalab.dfci.harvard.edu/dsbook/
